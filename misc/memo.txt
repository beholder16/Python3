'범위'가 현저히 다른 신호(시계열)들을 딥러닝을 이용해 예측하고 해석하는 일을 하고 있습니다. 

여러 실험들을 통해서, 신호에 적합한 scaling 없이는 모델이 잘 작동하지 않는다는 것을 알게됐습니다. 그래서 저는 현재는 Min-Max scaling 을 이용해 신호별로 전처리를 거친 후 모델을 훈련하고 역변환을 이용해 원래의 범위로 신호를 돌려놓고 결과를 해석하고 있습니다.

하지만 Min-Max scaling은 이름에서 알수 있듯, 장비의 측정오차나 다른 이유로, 최소, 최대값의 변동에 큰 영향을 받게 됩니다. 즉, 모델의 일반화 능력이 떨어지게 됩니다.

다른 고려한 점은 Z-transform 입니다. 정규분포를 이용해서 최대/최소값의 변동에 강인해지만, 평균값 근처에서 값들이 매우 높은 밀도로 분포하여, 변환된 값 (즉 z 값)을 이용해,regression 문제를 풀 때, 해상력이 떨어지는 단점이 있었습니다.

예외적으로, 입력단에 BN 레이어를 이용해 강제로 input distribution을 z-transform에 근사시키는 방법을 사용하면, 전처리 없이 어느 정도는 모델이 작동했으나, 정확도가 개선되지 않
았습니다.

혹시 저와 비슷한 문제를 가지고 있거나 관심이 있으신분들은 이야기를 나눠보면 좋겠습니다.

감사합니다.
